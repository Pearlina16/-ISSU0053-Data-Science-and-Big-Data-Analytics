---
title: "Classification"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*.

## Classification

This is an example of the general result for conditional probabilities:

P (A \| B) = P(A and B) / P(B)

The methodology for moving from ğ‘ƒ(ğ´\|ğµ) to ğ‘ƒ(ğµ\|ğ´) is known as *Bayesâ€™ theorem:*

ğ‘ƒ(ğµ\|ğ´) = ğ‘ƒ(ğ´\|ğµ)Ã—ğ‘ƒ(ğµ) / ğ‘ƒ(ğ´)

Bayesâ€™ theorem is the foundation of the major statistical subfield of Bayesian statistics.

## Linear regression

A standard linear regression model takes the form:ğ‘Œ=ğ›½0+ğ›½1ğ‘‹+ğœ–

Where ğœ– is independent of ğ‘‹ and ğ‘šğ‘’ğ‘ğ‘›(ğœ–)=0 if the assumptions underlying linear regression are satisfied.

Taking the expectation of ğ‘Œ conditional on a specific value of ğ‘‹ in the above expression gives:ğ¸[ğ‘Œ\|ğ‘‹=ğ‘¥]=ğ¸[ğ›½0+ğ›½1ğ‘‹+ğœ–\|ğ‘‹=ğ‘¥]=ğ¸[ğ›½0\|ğ‘‹=ğ‘¥]+ğ¸[ğ›½1ğ‘‹\|ğ‘‹=ğ‘¥]+ğ¸[ğœ–\|ğ‘‹=ğ‘¥]=ğ›½0+ğ›½1ğ‘¥Where we have made use of the fact that the expectation is a linear operator.

## **Expectation in the case of a binary outcome**

For a discrete random variable ğ‘Œ, we have that:ğ¸[ğ‘Œ]=âˆ‘ğ‘¦ğ‘¦Ã—ğ‘ƒ(ğ‘Œ=ğ‘¦)

In the case of a binary random variable, taking the values 0 and 1, we have that:ğ¸[ğ‘Œ]=âˆ‘ğ‘¦ğ‘¦Ã—ğ‘ƒ(ğ‘Œ=ğ‘¦)=0Ã—ğ‘ƒ(ğ‘Œ=0)+1Ã—ğ‘ƒ(ğ‘Œ=1)=ğ‘ƒ(ğ‘Œ=1)

eg. 0 male 1 female

## **Linear regression for classification into two categories**

Combining the two previous results, we may fit a linear regression model to an outcome ğ‘Œ and interpret the predictions provided by this model as conditional estimates of the outcome, ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥).

ğ¸[ğ‘Œ\|ğ‘‹=ğ‘¥]=ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)=ğ›½0+ğ›½1ğ‘¥

From this, we can interpret ğ›½0+ğ›½1ğ‘¥ as ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥). This result can be the basis of a classification procedure. If ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥) is large, we would classify observation ğ‘¥ in the category corresponding to ğ‘Œ=1, and vice versa.

## **Application**

The `mf_training.Rda` and `mf_test.Rda` files are R data files, which can be read directly into the working environment.

```{r}
library(tidyverse)
load("/Users/pp16/Desktop/NTU/UCL exchange/week 2 day 11/mf_training.Rda")
head(mf_training)

```

-   0/1 classification is important for our discussion to hold

```{r}
model_1 <- lm(gender ~ height, data=mf_training)

height_grid <- tibble(height=seq(140,200,length.out=100))
height_grid$gender <- predict(model_1, newdata=height_grid)
ggplot(mapping=aes(y=gender, x=height)) +
  geom_point(data=mf_training) +
  geom_line(data=height_grid)
```

-   we divide at 171: \>171: male, \<171: female

-   will have wrongly classified male and females

-   no line we can draw that perfectly distinguish

```{r}
summary(model_1)
```

We can view a summary of the model, to see that the estimated coefficients are ğ›½\^0=4.90 and ğ›½\^1=âˆ’0.03.

```{r}
db <- (0.5-model_1$coefficients[[1]])/model_1$coefficients[[2]]
db
```

Thus, if an individual has a height less than 171cm we might classify them as female, with individuals with a height greater than 171cm classified as male.

We can check the accuracy of our predictor.

```{r}
mf_training$pred <- predict(model_1)
mf_training$pred_gender <- if_else(mf_training$pred < 0.5, 0, 1)

table(predicted=mf_training$pred_gender,actual=mf_training$gender)
```

-   180 actual male were predicted to be male

-   521 actual female were correctly predicted to be female

-   diagonal is correctly labelled

```{r}
sum(mf_training$pred_gender!=mf_training$gender)/nrow(mf_training)
```

-   accuracy of our model

As with other approaches, a validation set approach may be more appropriate to quantify the accuracy of predictions in this case. We can therefore use the model built with the training data to make predictions for the test data and quantify the resulting accuracy.

```{r}
load("/Users/pp16/Desktop/NTU/UCL exchange/week 3 day 11/mf_test.Rda")
nrow(mf_test)

mf_test$pred <- predict(model_1, newdata=mf_test)
mf_test$pred_gender <- if_else(mf_test$pred < 0.5, 0, 1)

table(predicted=mf_test$pred_gender,actual=mf_test$gender)
```

```{r}
sum(mf_test$pred_gender!=mf_test$gender)/nrow(mf_test)
```

-   increase in error to 36%

# **Logistic regression**

An alternative might be to relate a function of ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥), ğ‘”(ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)) say, to the linear predictor ğ›½0+ğ›½1ğ‘¥. This is the foundation of the broader class of generalised linear models. Under this framework, ğ‘”() is referred to as the link function.

Logistic regression is one example of a generalised linear model which suits our requirements here. The link function is the logit function:ğ‘”(ğœ‡)=logâ¡(ğœ‡1âˆ’ğœ‡)The logit function takes values in [0,1] and transforms them to the range (âˆ’âˆ,âˆ).

Our logistic regression model is therefore of the form:logâ¡(ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)1âˆ’ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥))=ğ›½0+ğ›½1ğ‘¥The left hand side of this expression is commonly referred to as the log odds.

```{r}
mf_training %>%
  mutate(gender=factor(gender, labels=c("M","F"))) %>%
  ggplot() +
  geom_density(mapping=aes(x=height, color=gender))
```

-   bionomial: log odds (probability)

```{r}
model_logreg <- glm(gender ~ height,
                    data=mf_training,
                    family=binomial)

summary(model_logreg)
```

The R output indicates:logâ¡(ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)1âˆ’ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥))=21.1âˆ’0.1ğ‘¥

We often want to use a logistic regression model to make predictions for ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥), for which we can rearrange the general model expression.logâ¡(ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)1âˆ’ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥))=ğ›½0+ğ›½1ğ‘¥â‡’ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)=11+expâ¡(âˆ’(ğ›½0+ğ›½1ğ‘¥))The expression on the right hand side is the logistic function, the inverse of the logit function, which is how logistic regression gets its name.

```{r}
mf_fit <- tibble(height=seq(140,200,length.out=100))

mf_fit$gender <- predict(model_logreg,
                         newdata=mf_fit,
                         type="resp")

ggplot(mapping=aes(y=gender, x=height)) +
  geom_point(data=mf_training) +
  geom_line(data=mf_fit)
```

logâ¡(ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)1âˆ’ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥))=ğ›½0+ğ›½1ğ‘¥ğ‘ƒ(ğ‘Œ=1\|ğ‘‹=ğ‘¥)=0.5â‡’0=ğ›½0+ğ›½1ğ‘¥â‡’

ğ‘¥=âˆ’ğ›½0/ğ›½1

```{r}
	
  db
db_logreg <- (-model_logreg$coefficients[[1]])/model_logreg$coefficients[[2]]
db_logreg
```

```{r}
mf_training$pred <- predict(model_logreg,type="resp")
mf_training$pred_gender <- if_else(mf_training$pred < 0.5, 0, 1)

table(predicted=mf_training$pred_gender,actual=mf_training$gender)

```

```{r}
sum(mf_training$pred_gender!=mf_training$gender)/nrow(mf_training)

```

-   ?predict.glm

-   resp: type =\> response variable (0/1) probability

```{r}
mf_test$pred <- predict(model_logreg,type="resp",newdata=mf_test)
mf_test$pred_gender <- if_else(mf_test$pred < 0.5, 0, 1)

table(predicted=mf_test$pred_gender,actual=mf_test$gender)

```

# **Classification in the case of more than two outcome categories**

```{r}
data(iris)

ggplot(data=iris, mapping=aes(x=Petal.Length,
                              y=Petal.Width,
                              color=Species)) +
  geom_point()

```

```{r}
iris_simple <- iris %>%
  filter(Species == "virginica" | Species=="versicolor") %>%
  select(Species, Sepal.Length, Sepal.Width) %>%
  mutate(is_versicolor = if_else(Species == "versicolor", 1, 0),
         Species = droplevels(Species))

# note the drop levels command updates the factor levels
# to reflect it only contains entries for 2 Species

head(iris_simple)
```

```{r}
logistic.model <- glm( is_versicolor ~ Sepal.Length + Sepal.Width,
                    data=iris_simple,family=binomial)

logistic.model.prediction <- if_else( predict(logistic.model,type='resp') > 0.5,
                                      'versicolor',
                                      'virginica')

table(predicted=logistic.model.prediction,actual=iris_simple$Species)

```

```{r}
correct <- sum(logistic.model.prediction == iris_simple$Species)
correct
```

```{r}
incorrect <- sum(logistic.model.prediction != iris_simple$Species)
incorrect
```

```{r}
library(MASS)
select <- dplyr::select

lda.model <- lda(Species ~ Sepal.Length+Sepal.Width, data = iris_simple)

lda.model.prediction <- predict(lda.model)$class
table(predicted=lda.model.prediction,actual=iris_simple$Species)
```

```{r}
table(LDA=lda.model.prediction,LR=logistic.model.prediction)
```

```{r}
lda.model <- lda(Species ~ Sepal.Length+Sepal.Width, data = iris)

lda.model.prediction <- predict(lda.model)$class

table(predicted=lda.model.prediction,actual=iris$Species)
```

```{r}
correct <- sum(lda.model.prediction == iris$Species)
correct
```

```{r}
incorrect <- sum(lda.model.prediction != iris$Species)
incorrect
```

```{r}
qda.model <- qda(Species ~ Sepal.Length+Sepal.Width, data = iris)

qda.model.prediction <- predict(qda.model)$class

table(predicted=qda.model.prediction,actual=iris$Species)

```

```{r}
correct = sum(qda.model.prediction == iris$Species)
correct
incorrect = sum(qda.model.prediction != iris$Species)
incorrect
```
